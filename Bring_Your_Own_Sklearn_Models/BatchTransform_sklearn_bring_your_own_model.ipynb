{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring your own Scikit learn model to SageMaker for Batch Transform\n",
    "This tutorial shows you how to bring your [Scikit-learn](https://scikit-learn.org/stable/) models to SageMaker so that you can host and make inferences using SageMaker infrastructure. Scikit-learn is a popular Python machine learning framework. It includes a number of different algorithms for classification, regression, clustering, dimensionality reduction, and data/feature pre-processing. \n",
    "\n",
    "The [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) module makes it easy to take existing scikit-learn model and generate predictions using the SageMaker hosting and inferencing service. For more information about the Scikit-learn container, see the [sagemaker-scikit-learn-containers](https://github.com/aws/sagemaker-scikit-learn-container) repository and the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) repository. Note that the version of the sklearn used for training has to match that of provided container.\n",
    "\n",
    "For more on Scikit-learn, please visit the Scikit-learn website: <http://scikit-learn.org/stable/>.\n",
    "\n",
    "### Table of contents\n",
    "* [Upload the data for training](#upload_data)\n",
    "* [Create a Scikit-learn script to train with](#create_sklearn_script)\n",
    "* [Batch Transform](#batch_transform)\n",
    " * [Prepare Input Data](#prepare_input_data)\n",
    " * [Run Transform Job](#run_transform_job)\n",
    " * [Check Output Data](#check_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 2.9 MB/s eta 0:00:01     |▋                               | 122 kB 2.9 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (1.18.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (0.14.1)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'Scikit-iris'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"upload_data\"></a>\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using a sample of the classic [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which is included with Scikit-learn. We will load the dataset, write locally, then write the dataset to s3 to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load Iris dataset, then join labels and features\n",
    "iris = datasets.load_iris()\n",
    "joined_iris = np.insert(iris.data, 0, iris.target, axis=1)\n",
    "\n",
    "# Create directory and write csv\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "np.savetxt('./data/iris.csv', joined_iris, delimiter=',', fmt='%1.1f, %1.3f, %1.3f, %1.3f, %1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0. , 5.1, 3.5, 1.4, 0.2],\n",
       "        [0. , 4.9, 3. , 1.4, 0.2],\n",
       "        [0. , 4.7, 3.2, 1.3, 0.2],\n",
       "        [0. , 4.6, 3.1, 1.5, 0.2],\n",
       "        [0. , 5. , 3.6, 1.4, 0.2],\n",
       "        [0. , 5.4, 3.9, 1.7, 0.4],\n",
       "        [0. , 4.6, 3.4, 1.4, 0.3],\n",
       "        [0. , 5. , 3.4, 1.5, 0.2],\n",
       "        [0. , 4.4, 2.9, 1.4, 0.2],\n",
       "        [0. , 4.9, 3.1, 1.5, 0.1],\n",
       "        [0. , 5.4, 3.7, 1.5, 0.2],\n",
       "        [0. , 4.8, 3.4, 1.6, 0.2],\n",
       "        [0. , 4.8, 3. , 1.4, 0.1],\n",
       "        [0. , 4.3, 3. , 1.1, 0.1],\n",
       "        [0. , 5.8, 4. , 1.2, 0.2],\n",
       "        [0. , 5.7, 4.4, 1.5, 0.4],\n",
       "        [0. , 5.4, 3.9, 1.3, 0.4],\n",
       "        [0. , 5.1, 3.5, 1.4, 0.3],\n",
       "        [0. , 5.7, 3.8, 1.7, 0.3],\n",
       "        [0. , 5.1, 3.8, 1.5, 0.3],\n",
       "        [0. , 5.4, 3.4, 1.7, 0.2],\n",
       "        [0. , 5.1, 3.7, 1.5, 0.4],\n",
       "        [0. , 4.6, 3.6, 1. , 0.2],\n",
       "        [0. , 5.1, 3.3, 1.7, 0.5],\n",
       "        [0. , 4.8, 3.4, 1.9, 0.2],\n",
       "        [0. , 5. , 3. , 1.6, 0.2],\n",
       "        [0. , 5. , 3.4, 1.6, 0.4],\n",
       "        [0. , 5.2, 3.5, 1.5, 0.2],\n",
       "        [0. , 5.2, 3.4, 1.4, 0.2],\n",
       "        [0. , 4.7, 3.2, 1.6, 0.2],\n",
       "        [0. , 4.8, 3.1, 1.6, 0.2],\n",
       "        [0. , 5.4, 3.4, 1.5, 0.4],\n",
       "        [0. , 5.2, 4.1, 1.5, 0.1],\n",
       "        [0. , 5.5, 4.2, 1.4, 0.2],\n",
       "        [0. , 4.9, 3.1, 1.5, 0.2],\n",
       "        [0. , 5. , 3.2, 1.2, 0.2],\n",
       "        [0. , 5.5, 3.5, 1.3, 0.2],\n",
       "        [0. , 4.9, 3.6, 1.4, 0.1],\n",
       "        [0. , 4.4, 3. , 1.3, 0.2],\n",
       "        [0. , 5.1, 3.4, 1.5, 0.2],\n",
       "        [0. , 5. , 3.5, 1.3, 0.3],\n",
       "        [0. , 4.5, 2.3, 1.3, 0.3],\n",
       "        [0. , 4.4, 3.2, 1.3, 0.2],\n",
       "        [0. , 5. , 3.5, 1.6, 0.6],\n",
       "        [0. , 5.1, 3.8, 1.9, 0.4],\n",
       "        [0. , 4.8, 3. , 1.4, 0.3],\n",
       "        [0. , 5.1, 3.8, 1.6, 0.2],\n",
       "        [0. , 4.6, 3.2, 1.4, 0.2],\n",
       "        [0. , 5.3, 3.7, 1.5, 0.2],\n",
       "        [0. , 5. , 3.3, 1.4, 0.2],\n",
       "        [1. , 7. , 3.2, 4.7, 1.4],\n",
       "        [1. , 6.4, 3.2, 4.5, 1.5],\n",
       "        [1. , 6.9, 3.1, 4.9, 1.5],\n",
       "        [1. , 5.5, 2.3, 4. , 1.3],\n",
       "        [1. , 6.5, 2.8, 4.6, 1.5],\n",
       "        [1. , 5.7, 2.8, 4.5, 1.3],\n",
       "        [1. , 6.3, 3.3, 4.7, 1.6],\n",
       "        [1. , 4.9, 2.4, 3.3, 1. ],\n",
       "        [1. , 6.6, 2.9, 4.6, 1.3],\n",
       "        [1. , 5.2, 2.7, 3.9, 1.4],\n",
       "        [1. , 5. , 2. , 3.5, 1. ],\n",
       "        [1. , 5.9, 3. , 4.2, 1.5],\n",
       "        [1. , 6. , 2.2, 4. , 1. ],\n",
       "        [1. , 6.1, 2.9, 4.7, 1.4],\n",
       "        [1. , 5.6, 2.9, 3.6, 1.3],\n",
       "        [1. , 6.7, 3.1, 4.4, 1.4],\n",
       "        [1. , 5.6, 3. , 4.5, 1.5],\n",
       "        [1. , 5.8, 2.7, 4.1, 1. ],\n",
       "        [1. , 6.2, 2.2, 4.5, 1.5],\n",
       "        [1. , 5.6, 2.5, 3.9, 1.1],\n",
       "        [1. , 5.9, 3.2, 4.8, 1.8],\n",
       "        [1. , 6.1, 2.8, 4. , 1.3],\n",
       "        [1. , 6.3, 2.5, 4.9, 1.5],\n",
       "        [1. , 6.1, 2.8, 4.7, 1.2],\n",
       "        [1. , 6.4, 2.9, 4.3, 1.3],\n",
       "        [1. , 6.6, 3. , 4.4, 1.4],\n",
       "        [1. , 6.8, 2.8, 4.8, 1.4],\n",
       "        [1. , 6.7, 3. , 5. , 1.7],\n",
       "        [1. , 6. , 2.9, 4.5, 1.5],\n",
       "        [1. , 5.7, 2.6, 3.5, 1. ],\n",
       "        [1. , 5.5, 2.4, 3.8, 1.1],\n",
       "        [1. , 5.5, 2.4, 3.7, 1. ],\n",
       "        [1. , 5.8, 2.7, 3.9, 1.2],\n",
       "        [1. , 6. , 2.7, 5.1, 1.6],\n",
       "        [1. , 5.4, 3. , 4.5, 1.5],\n",
       "        [1. , 6. , 3.4, 4.5, 1.6],\n",
       "        [1. , 6.7, 3.1, 4.7, 1.5],\n",
       "        [1. , 6.3, 2.3, 4.4, 1.3],\n",
       "        [1. , 5.6, 3. , 4.1, 1.3],\n",
       "        [1. , 5.5, 2.5, 4. , 1.3],\n",
       "        [1. , 5.5, 2.6, 4.4, 1.2],\n",
       "        [1. , 6.1, 3. , 4.6, 1.4],\n",
       "        [1. , 5.8, 2.6, 4. , 1.2],\n",
       "        [1. , 5. , 2.3, 3.3, 1. ],\n",
       "        [1. , 5.6, 2.7, 4.2, 1.3],\n",
       "        [1. , 5.7, 3. , 4.2, 1.2],\n",
       "        [1. , 5.7, 2.9, 4.2, 1.3],\n",
       "        [1. , 6.2, 2.9, 4.3, 1.3],\n",
       "        [1. , 5.1, 2.5, 3. , 1.1],\n",
       "        [1. , 5.7, 2.8, 4.1, 1.3],\n",
       "        [2. , 6.3, 3.3, 6. , 2.5],\n",
       "        [2. , 5.8, 2.7, 5.1, 1.9],\n",
       "        [2. , 7.1, 3. , 5.9, 2.1],\n",
       "        [2. , 6.3, 2.9, 5.6, 1.8],\n",
       "        [2. , 6.5, 3. , 5.8, 2.2],\n",
       "        [2. , 7.6, 3. , 6.6, 2.1],\n",
       "        [2. , 4.9, 2.5, 4.5, 1.7],\n",
       "        [2. , 7.3, 2.9, 6.3, 1.8],\n",
       "        [2. , 6.7, 2.5, 5.8, 1.8],\n",
       "        [2. , 7.2, 3.6, 6.1, 2.5],\n",
       "        [2. , 6.5, 3.2, 5.1, 2. ],\n",
       "        [2. , 6.4, 2.7, 5.3, 1.9],\n",
       "        [2. , 6.8, 3. , 5.5, 2.1],\n",
       "        [2. , 5.7, 2.5, 5. , 2. ],\n",
       "        [2. , 5.8, 2.8, 5.1, 2.4],\n",
       "        [2. , 6.4, 3.2, 5.3, 2.3],\n",
       "        [2. , 6.5, 3. , 5.5, 1.8],\n",
       "        [2. , 7.7, 3.8, 6.7, 2.2],\n",
       "        [2. , 7.7, 2.6, 6.9, 2.3],\n",
       "        [2. , 6. , 2.2, 5. , 1.5],\n",
       "        [2. , 6.9, 3.2, 5.7, 2.3],\n",
       "        [2. , 5.6, 2.8, 4.9, 2. ],\n",
       "        [2. , 7.7, 2.8, 6.7, 2. ],\n",
       "        [2. , 6.3, 2.7, 4.9, 1.8],\n",
       "        [2. , 6.7, 3.3, 5.7, 2.1],\n",
       "        [2. , 7.2, 3.2, 6. , 1.8],\n",
       "        [2. , 6.2, 2.8, 4.8, 1.8],\n",
       "        [2. , 6.1, 3. , 4.9, 1.8],\n",
       "        [2. , 6.4, 2.8, 5.6, 2.1],\n",
       "        [2. , 7.2, 3. , 5.8, 1.6],\n",
       "        [2. , 7.4, 2.8, 6.1, 1.9],\n",
       "        [2. , 7.9, 3.8, 6.4, 2. ],\n",
       "        [2. , 6.4, 2.8, 5.6, 2.2],\n",
       "        [2. , 6.3, 2.8, 5.1, 1.5],\n",
       "        [2. , 6.1, 2.6, 5.6, 1.4],\n",
       "        [2. , 7.7, 3. , 6.1, 2.3],\n",
       "        [2. , 6.3, 3.4, 5.6, 2.4],\n",
       "        [2. , 6.4, 3.1, 5.5, 1.8],\n",
       "        [2. , 6. , 3. , 4.8, 1.8],\n",
       "        [2. , 6.9, 3.1, 5.4, 2.1],\n",
       "        [2. , 6.7, 3.1, 5.6, 2.4],\n",
       "        [2. , 6.9, 3.1, 5.1, 2.3],\n",
       "        [2. , 5.8, 2.7, 5.1, 1.9],\n",
       "        [2. , 6.8, 3.2, 5.9, 2.3],\n",
       "        [2. , 6.7, 3.3, 5.7, 2.5],\n",
       "        [2. , 6.7, 3. , 5.2, 2.3],\n",
       "        [2. , 6.3, 2.5, 5. , 1.9],\n",
       "        [2. , 6.5, 3. , 5.2, 2. ],\n",
       "        [2. , 6.2, 3.4, 5.4, 2.3],\n",
       "        [2. , 5.9, 3. , 5.1, 1.8]]),\n",
       " {'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "         [4.9, 3. , 1.4, 0.2],\n",
       "         [4.7, 3.2, 1.3, 0.2],\n",
       "         [4.6, 3.1, 1.5, 0.2],\n",
       "         [5. , 3.6, 1.4, 0.2],\n",
       "         [5.4, 3.9, 1.7, 0.4],\n",
       "         [4.6, 3.4, 1.4, 0.3],\n",
       "         [5. , 3.4, 1.5, 0.2],\n",
       "         [4.4, 2.9, 1.4, 0.2],\n",
       "         [4.9, 3.1, 1.5, 0.1],\n",
       "         [5.4, 3.7, 1.5, 0.2],\n",
       "         [4.8, 3.4, 1.6, 0.2],\n",
       "         [4.8, 3. , 1.4, 0.1],\n",
       "         [4.3, 3. , 1.1, 0.1],\n",
       "         [5.8, 4. , 1.2, 0.2],\n",
       "         [5.7, 4.4, 1.5, 0.4],\n",
       "         [5.4, 3.9, 1.3, 0.4],\n",
       "         [5.1, 3.5, 1.4, 0.3],\n",
       "         [5.7, 3.8, 1.7, 0.3],\n",
       "         [5.1, 3.8, 1.5, 0.3],\n",
       "         [5.4, 3.4, 1.7, 0.2],\n",
       "         [5.1, 3.7, 1.5, 0.4],\n",
       "         [4.6, 3.6, 1. , 0.2],\n",
       "         [5.1, 3.3, 1.7, 0.5],\n",
       "         [4.8, 3.4, 1.9, 0.2],\n",
       "         [5. , 3. , 1.6, 0.2],\n",
       "         [5. , 3.4, 1.6, 0.4],\n",
       "         [5.2, 3.5, 1.5, 0.2],\n",
       "         [5.2, 3.4, 1.4, 0.2],\n",
       "         [4.7, 3.2, 1.6, 0.2],\n",
       "         [4.8, 3.1, 1.6, 0.2],\n",
       "         [5.4, 3.4, 1.5, 0.4],\n",
       "         [5.2, 4.1, 1.5, 0.1],\n",
       "         [5.5, 4.2, 1.4, 0.2],\n",
       "         [4.9, 3.1, 1.5, 0.2],\n",
       "         [5. , 3.2, 1.2, 0.2],\n",
       "         [5.5, 3.5, 1.3, 0.2],\n",
       "         [4.9, 3.6, 1.4, 0.1],\n",
       "         [4.4, 3. , 1.3, 0.2],\n",
       "         [5.1, 3.4, 1.5, 0.2],\n",
       "         [5. , 3.5, 1.3, 0.3],\n",
       "         [4.5, 2.3, 1.3, 0.3],\n",
       "         [4.4, 3.2, 1.3, 0.2],\n",
       "         [5. , 3.5, 1.6, 0.6],\n",
       "         [5.1, 3.8, 1.9, 0.4],\n",
       "         [4.8, 3. , 1.4, 0.3],\n",
       "         [5.1, 3.8, 1.6, 0.2],\n",
       "         [4.6, 3.2, 1.4, 0.2],\n",
       "         [5.3, 3.7, 1.5, 0.2],\n",
       "         [5. , 3.3, 1.4, 0.2],\n",
       "         [7. , 3.2, 4.7, 1.4],\n",
       "         [6.4, 3.2, 4.5, 1.5],\n",
       "         [6.9, 3.1, 4.9, 1.5],\n",
       "         [5.5, 2.3, 4. , 1.3],\n",
       "         [6.5, 2.8, 4.6, 1.5],\n",
       "         [5.7, 2.8, 4.5, 1.3],\n",
       "         [6.3, 3.3, 4.7, 1.6],\n",
       "         [4.9, 2.4, 3.3, 1. ],\n",
       "         [6.6, 2.9, 4.6, 1.3],\n",
       "         [5.2, 2.7, 3.9, 1.4],\n",
       "         [5. , 2. , 3.5, 1. ],\n",
       "         [5.9, 3. , 4.2, 1.5],\n",
       "         [6. , 2.2, 4. , 1. ],\n",
       "         [6.1, 2.9, 4.7, 1.4],\n",
       "         [5.6, 2.9, 3.6, 1.3],\n",
       "         [6.7, 3.1, 4.4, 1.4],\n",
       "         [5.6, 3. , 4.5, 1.5],\n",
       "         [5.8, 2.7, 4.1, 1. ],\n",
       "         [6.2, 2.2, 4.5, 1.5],\n",
       "         [5.6, 2.5, 3.9, 1.1],\n",
       "         [5.9, 3.2, 4.8, 1.8],\n",
       "         [6.1, 2.8, 4. , 1.3],\n",
       "         [6.3, 2.5, 4.9, 1.5],\n",
       "         [6.1, 2.8, 4.7, 1.2],\n",
       "         [6.4, 2.9, 4.3, 1.3],\n",
       "         [6.6, 3. , 4.4, 1.4],\n",
       "         [6.8, 2.8, 4.8, 1.4],\n",
       "         [6.7, 3. , 5. , 1.7],\n",
       "         [6. , 2.9, 4.5, 1.5],\n",
       "         [5.7, 2.6, 3.5, 1. ],\n",
       "         [5.5, 2.4, 3.8, 1.1],\n",
       "         [5.5, 2.4, 3.7, 1. ],\n",
       "         [5.8, 2.7, 3.9, 1.2],\n",
       "         [6. , 2.7, 5.1, 1.6],\n",
       "         [5.4, 3. , 4.5, 1.5],\n",
       "         [6. , 3.4, 4.5, 1.6],\n",
       "         [6.7, 3.1, 4.7, 1.5],\n",
       "         [6.3, 2.3, 4.4, 1.3],\n",
       "         [5.6, 3. , 4.1, 1.3],\n",
       "         [5.5, 2.5, 4. , 1.3],\n",
       "         [5.5, 2.6, 4.4, 1.2],\n",
       "         [6.1, 3. , 4.6, 1.4],\n",
       "         [5.8, 2.6, 4. , 1.2],\n",
       "         [5. , 2.3, 3.3, 1. ],\n",
       "         [5.6, 2.7, 4.2, 1.3],\n",
       "         [5.7, 3. , 4.2, 1.2],\n",
       "         [5.7, 2.9, 4.2, 1.3],\n",
       "         [6.2, 2.9, 4.3, 1.3],\n",
       "         [5.1, 2.5, 3. , 1.1],\n",
       "         [5.7, 2.8, 4.1, 1.3],\n",
       "         [6.3, 3.3, 6. , 2.5],\n",
       "         [5.8, 2.7, 5.1, 1.9],\n",
       "         [7.1, 3. , 5.9, 2.1],\n",
       "         [6.3, 2.9, 5.6, 1.8],\n",
       "         [6.5, 3. , 5.8, 2.2],\n",
       "         [7.6, 3. , 6.6, 2.1],\n",
       "         [4.9, 2.5, 4.5, 1.7],\n",
       "         [7.3, 2.9, 6.3, 1.8],\n",
       "         [6.7, 2.5, 5.8, 1.8],\n",
       "         [7.2, 3.6, 6.1, 2.5],\n",
       "         [6.5, 3.2, 5.1, 2. ],\n",
       "         [6.4, 2.7, 5.3, 1.9],\n",
       "         [6.8, 3. , 5.5, 2.1],\n",
       "         [5.7, 2.5, 5. , 2. ],\n",
       "         [5.8, 2.8, 5.1, 2.4],\n",
       "         [6.4, 3.2, 5.3, 2.3],\n",
       "         [6.5, 3. , 5.5, 1.8],\n",
       "         [7.7, 3.8, 6.7, 2.2],\n",
       "         [7.7, 2.6, 6.9, 2.3],\n",
       "         [6. , 2.2, 5. , 1.5],\n",
       "         [6.9, 3.2, 5.7, 2.3],\n",
       "         [5.6, 2.8, 4.9, 2. ],\n",
       "         [7.7, 2.8, 6.7, 2. ],\n",
       "         [6.3, 2.7, 4.9, 1.8],\n",
       "         [6.7, 3.3, 5.7, 2.1],\n",
       "         [7.2, 3.2, 6. , 1.8],\n",
       "         [6.2, 2.8, 4.8, 1.8],\n",
       "         [6.1, 3. , 4.9, 1.8],\n",
       "         [6.4, 2.8, 5.6, 2.1],\n",
       "         [7.2, 3. , 5.8, 1.6],\n",
       "         [7.4, 2.8, 6.1, 1.9],\n",
       "         [7.9, 3.8, 6.4, 2. ],\n",
       "         [6.4, 2.8, 5.6, 2.2],\n",
       "         [6.3, 2.8, 5.1, 1.5],\n",
       "         [6.1, 2.6, 5.6, 1.4],\n",
       "         [7.7, 3. , 6.1, 2.3],\n",
       "         [6.3, 3.4, 5.6, 2.4],\n",
       "         [6.4, 3.1, 5.5, 1.8],\n",
       "         [6. , 3. , 4.8, 1.8],\n",
       "         [6.9, 3.1, 5.4, 2.1],\n",
       "         [6.7, 3.1, 5.6, 2.4],\n",
       "         [6.9, 3.1, 5.1, 2.3],\n",
       "         [5.8, 2.7, 5.1, 1.9],\n",
       "         [6.8, 3.2, 5.9, 2.3],\n",
       "         [6.7, 3.3, 5.7, 2.5],\n",
       "         [6.7, 3. , 5.2, 2.3],\n",
       "         [6.3, 2.5, 5. , 1.9],\n",
       "         [6.5, 3. , 5.2, 2. ],\n",
       "         [6.2, 3.4, 5.4, 2.3],\n",
       "         [5.9, 3. , 5.1, 1.8]]),\n",
       "  'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       "  'frame': None,\n",
       "  'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       "  'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       "  'feature_names': ['sepal length (cm)',\n",
       "   'sepal width (cm)',\n",
       "   'petal length (cm)',\n",
       "   'petal width (cm)'],\n",
       "  'filename': '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/datasets/data/iris.csv'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_iris, iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dataset built up, we can start the sklearn ML training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = joined_iris[:, 0] # label\n",
    "train_X = joined_iris[:, 1:] # features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Model trains very quickly because the data is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=30)\n",
    "clf = clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the classifier `clf` trained, we can save it using [joblib](https://scikit-learn.org/stable/modules/model_persistence.html) which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators. The model needs to be archived into a file `model.tar.gz` and saved to S3 to be used as a SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf, 'model.joblib')\n",
    "\n",
    "!tar -czf model.tar.gz model.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='s3://%s/%s/output/model.tar.gz' % (sagemaker_session.default_bucket(), prefix)\n",
    "!aws s3 cp model.tar.gz {model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get inferences for an entire dataset, use batch transform. With batch transform, you create a batch transform job using a trained model and the dataset, which must be stored in Amazon S3. Amazon SageMaker saves the inferences in an S3 bucket that you specify when you create the batch transform job. Batch transform manages all of the compute resources required to get inferences. This includes launching instances and deleting them after the batch transform job has completed. Batch transform manages interactions between the data and the model with an object within the instance node called an agent.\n",
    "\n",
    "Use batch transform when you:\n",
    "\n",
    "- Want to get inferences for an entire dataset and index them to serve inferences in real time\n",
    "\n",
    "- Don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "\n",
    "- Don't need the subsecond latency that Amazon SageMaker hosted endpoints provide\n",
    "\n",
    "You can also use batch transform to preprocess your data before using it to train a new model or generate inferences.\n",
    "\n",
    "The following diagram shows [the workflow of a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html):\n",
    "\n",
    "![image](https://docs.aws.amazon.com/sagemaker/latest/dg/images/batch-transform-v2.png)\n",
    "\n",
    "Now we have the classifier packaged up and are almost ready to use the model as a SageMaker model for inferencing. We would also need the serving codes that goes along with the classifier `model.tar.gz`. The serving script that will be put into the serving sklearn container requires an input argument `--model-dir` so that SageMaker can pass in the model location, and a function `model_fn` to deserialize and return the fitted classifier. We create a script below and save it as `sklearn_iris_serving.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sklearn_iris_serving.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sklearn_iris_serving.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "#     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a SageMaker Model for sklearn model. We need the model location (s3 path), SageMaker execution role, serving script as `entry_point` and the sklearn version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "sklearn_model = SKLearnModel(model_data=model,\n",
    "                             role=role,\n",
    "                             entry_point=\"sklearn_iris_serving.py\",\n",
    "                             framework_version=\"0.23-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform <a class=\"anchor\" id=\"batch_transform\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the sklearn model\n",
    "transformer = sklearn_model.transformer(instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Input Data <a class=\"anchor\" id=\"prepare_input_data\"></a>\n",
    "We will extract 10 random samples of 100 rows from the training data, then split the features (X) from the labels (Y). Then upload the input data to a given location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Randomly sample the iris dataset 10 times, then split X and Y\n",
    "mkdir -p batch_data/XY batch_data/X batch_data/Y\n",
    "for i in {0..9}; do\n",
    "    cat data/iris.csv | shuf -n 100 > batch_data/XY/iris_sample_${i}.csv\n",
    "    cat batch_data/XY/iris_sample_${i}.csv | cut -d',' -f2- > batch_data/X/iris_sample_X_${i}.csv\n",
    "    cat batch_data/XY/iris_sample_${i}.csv | cut -d',' -f1 > batch_data/Y/iris_sample_Y_${i}.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload input data from local filesystem to S3\n",
    "batch_input_s3 = sagemaker_session.upload_data('batch_data/X', key_prefix=prefix + '/batch_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job <a class=\"anchor\" id=\"run_transform_job\"></a>\n",
    "Using the Transformer, run a transform job on the S3 input data. `content_type` indicates that the input `batch_input_s3` is of csv type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for transform job: sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075\n",
      ".....................\u001b[34m2020-08-06 23:39:01,441 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:01,443 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:01,444 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:01,594 INFO - sagemaker-containers - Module sklearn_iris_serving does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:01,594 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:01,594 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:01,594 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-iris-serving\n",
      "  Building wheel for sklearn-iris-serving (setup.py): started\n",
      "  Building wheel for sklearn-iris-serving (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-iris-serving: filename=sklearn_iris_serving-1.0.0-py2.py3-none-any.whl size=5358 sha256=2d5eb6fde4fc84e4792474e7a496e60abe6695754a222c299f762061338436c1\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-tdu8q3s6/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-iris-serving\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-iris-serving\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-iris-serving-1.0.0\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [34] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [34] [INFO] Listening at: unix:/tmp/gunicorn.sock (34)\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [34] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-08-06 23:39:03 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:19,567 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:20,082 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:20 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:21 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:21,128 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:21 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:21 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2020-08-06 23:39:21,604 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:22 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [06/Aug/2020:23:39:22 +0000] \"POST /invocations HTTP/1.1\" 200 500 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2020-08-06T23:39:20.498:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start a transform job and wait for it to finish\n",
    "transformer.transform(batch_input_s3, content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Output Data  <a class=\"anchor\" id=\"check_output_data\"></a>\n",
    "After the transform job has completed, download the output data from S3. For each file \"f\" in the input data, we have a corresponding file \"f.out\" containing the predicted labels from each input row. We can compare the predicted labels to the true labels saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_0.csv.out to batch_data/output/iris_sample_X_0.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_5.csv.out to batch_data/output/iris_sample_X_5.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_1.csv.out to batch_data/output/iris_sample_X_1.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_2.csv.out to batch_data/output/iris_sample_X_2.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_4.csv.out to batch_data/output/iris_sample_X_4.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_3.csv.out to batch_data/output/iris_sample_X_3.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_6.csv.out to batch_data/output/iris_sample_X_6.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_9.csv.out to batch_data/output/iris_sample_X_9.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_8.csv.out to batch_data/output/iris_sample_X_8.csv.out\n",
      "download: s3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075/iris_sample_X_7.csv.out to batch_data/output/iris_sample_X_7.csv.out\n",
      "==> batch_data/output/iris_sample_X_0.csv.out <==\n",
      "[2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]\n",
      "==> batch_data/output/iris_sample_X_1.csv.out <==\n",
      "[0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0]\n",
      "==> batch_data/output/iris_sample_X_2.csv.out <==\n",
      "[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 1.0, 0.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0]\n",
      "==> batch_data/output/iris_sample_X_3.csv.out <==\n",
      "[1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0]\n",
      "==> batch_data/output/iris_sample_X_4.csv.out <==\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0]\n",
      "==> batch_data/output/iris_sample_X_5.csv.out <==\n",
      "[0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "==> batch_data/output/iris_sample_X_6.csv.out <==\n",
      "[2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 2.0, 2.0, 1.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 1.0, 1.0, 2.0]\n",
      "==> batch_data/output/iris_sample_X_7.csv.out <==\n",
      "[0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "==> batch_data/output/iris_sample_X_8.csv.out <==\n",
      "[0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 2.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0]\n",
      "==> batch_data/output/iris_sample_X_9.csv.out <==\n",
      "[2.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0]"
     ]
    }
   ],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/\n",
    "# Head to see what the batch output looks like\n",
    "!head batch_data/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files batch_data/Y/iris_sample_Y_1.csv and batch_data/output/iris_sample_X_1.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_2.csv and batch_data/output/iris_sample_X_2.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_3.csv and batch_data/output/iris_sample_X_3.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_4.csv and batch_data/output/iris_sample_X_4.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_5.csv and batch_data/output/iris_sample_X_5.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_6.csv and batch_data/output/iris_sample_X_6.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_7.csv and batch_data/output/iris_sample_X_7.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_8.csv and batch_data/output/iris_sample_X_8.csv.out are identical\n",
      "Files batch_data/Y/iris_sample_Y_9.csv and batch_data/output/iris_sample_X_9.csv.out are identical\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# For each sample file, compare the predicted labels from batch output to the true labels\n",
    "for i in {1..9}; do\n",
    "    diff -s batch_data/Y/iris_sample_Y_${i}.csv \\\n",
    "        <(cat batch_data/output/iris_sample_X_${i}.csv.out | sed 's/[[\"]//g' | sed 's/, \\|]/\\n/g') \\\n",
    "        | sed \"s/\\/dev\\/fd\\/63/batch_data\\/output\\/iris_sample_X_${i}.csv.out/\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing job information\n",
    "You can access retrospectively the batch transformation job information such as time of execution, model version, input data from the [SageMaker console](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-east-1#/transform-jobs) or from boto3 using [describe_transform_job](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.describe_transform_job) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client=boto3.client('sagemaker')\n",
    "result=sm_client.describe_transform_job(\n",
    "            TransformJobName=transformer.latest_transform_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-scikit-learn-2020-08-06-23-35-42-199'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model used\n",
    "result['ModelName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "   'S3Uri': 's3://sagemaker-us-west-2-029454422462/Scikit-iris/batch_input'}},\n",
       " 'ContentType': 'text/csv',\n",
       " 'CompressionType': 'None',\n",
       " 'SplitType': 'None'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data\n",
    "result['TransformInput']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S3OutputPath': 's3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075',\n",
       " 'AssembleWith': 'None',\n",
       " 'KmsKeyId': ''}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output result\n",
    "result['TransformOutput']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransformJobName': 'sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075',\n",
       " 'TransformJobArn': 'arn:aws:sagemaker:us-west-2:029454422462:transform-job/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075',\n",
       " 'TransformJobStatus': 'Completed',\n",
       " 'ModelName': 'sagemaker-scikit-learn-2020-08-06-23-35-42-199',\n",
       " 'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "    'S3Uri': 's3://sagemaker-us-west-2-029454422462/Scikit-iris/batch_input'}},\n",
       "  'ContentType': 'text/csv',\n",
       "  'CompressionType': 'None',\n",
       "  'SplitType': 'None'},\n",
       " 'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-2020-08-06-23-35-45-075',\n",
       "  'AssembleWith': 'None',\n",
       "  'KmsKeyId': ''},\n",
       " 'TransformResources': {'InstanceType': 'ml.m5.xlarge', 'InstanceCount': 1},\n",
       " 'CreationTime': datetime.datetime(2020, 8, 6, 23, 35, 45, 252000, tzinfo=tzlocal()),\n",
       " 'TransformStartTime': datetime.datetime(2020, 8, 6, 23, 37, 2, tzinfo=tzlocal()),\n",
       " 'TransformEndTime': datetime.datetime(2020, 8, 6, 23, 39, 22, tzinfo=tzlocal()),\n",
       " 'DataProcessing': {'InputFilter': '$',\n",
       "  'OutputFilter': '$',\n",
       "  'JoinSource': 'None'},\n",
       " 'ResponseMetadata': {'RequestId': '726845c1-8fc0-4e63-be1c-958a3997feba',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '726845c1-8fc0-4e63-be1c-958a3997feba',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '957',\n",
       "   'date': 'Fri, 07 Aug 2020 00:29:57 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelName': 'sagemaker-scikit-learn-2020-08-06-23-35-42-199',\n",
       " 'PrimaryContainer': {'Image': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "  'Mode': 'SingleModel',\n",
       "  'ModelDataUrl': 's3://sagemaker-us-west-2-029454422462/Scikit-iris/output/model.tar.gz',\n",
       "  'Environment': {'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "   'SAGEMAKER_ENABLE_CLOUDWATCH_METRICS': 'false',\n",
       "   'SAGEMAKER_PROGRAM': 'sklearn_iris_serving.py',\n",
       "   'SAGEMAKER_REGION': 'us-west-2',\n",
       "   'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-us-west-2-029454422462/sagemaker-scikit-learn-2020-08-06-23-35-41-900/sourcedir.tar.gz'}},\n",
       " 'ExecutionRoleArn': 'arn:aws:iam::029454422462:role/service-role/AmazonSageMaker-ExecutionRole-20191112T221060',\n",
       " 'CreationTime': datetime.datetime(2020, 8, 6, 23, 35, 42, 404000, tzinfo=tzlocal()),\n",
       " 'ModelArn': 'arn:aws:sagemaker:us-west-2:029454422462:model/sagemaker-scikit-learn-2020-08-06-23-35-42-199',\n",
       " 'EnableNetworkIsolation': False,\n",
       " 'ResponseMetadata': {'RequestId': '8c6c0587-f16b-43ee-9238-7b2e2d7fc203',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8c6c0587-f16b-43ee-9238-7b2e2d7fc203',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '874',\n",
       "   'date': 'Fri, 07 Aug 2020 00:34:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More information about the model\n",
    "model_result=sm_client.describe_model(ModelName=result['ModelName'])\n",
    "model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
