{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring your own Scikit learn model to SageMaker for Batch Transform\n",
    "This tutorial shows you how to bring your [Scikit-learn](https://scikit-learn.org/stable/) models to SageMaker so that you can host and make inferences using SageMaker infrastructure. Scikit-learn is a popular Python machine learning framework. It includes a number of different algorithms for classification, regression, clustering, dimensionality reduction, and data/feature pre-processing. \n",
    "\n",
    "The [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) module makes it easy to take existing scikit-learn model and generate predictions using the SageMaker hosting and inferencing service. For more information about the Scikit-learn container, see the [sagemaker-scikit-learn-containers](https://github.com/aws/sagemaker-scikit-learn-container) repository and the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) repository. Note that the version of the sklearn used for training has to match that of provided container.\n",
    "\n",
    "For more on Scikit-learn, please visit the Scikit-learn website: <http://scikit-learn.org/stable/>.\n",
    "\n",
    "### Table of contents\n",
    "* [Upload the data for training](#upload_data)\n",
    "* [Batch Transform](#batch_transform)\n",
    " * [Prepare Input Data](#prepare_input_data)\n",
    " * [Run Transform Job](#run_transform_job)\n",
    " * [Check Output Data](#check_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn==0.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'Scikit-two-outputs'\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"upload_data\"></a>\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're creating a sample classification dataset. We will load the dataset, write locally, then write the dataset to s3 to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy import sparse\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=90000, n_features=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dataset built up, we can start the sklearn ML training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=30)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the classifier `clf` trained, we can save it using `pickle` for fitted scikit-learn estimators. The model needs to be archived into a file `model.tar.gz` and saved to S3 to be used as a SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open('model.pkl', 'wb'))\n",
    "!tar -czf model.tar.gz model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -tzvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='s3://%s/%s/output/model.tar.gz' % (sagemaker_session.default_bucket(), prefix)\n",
    "!aws s3 cp model.tar.gz {model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get inferences for an entire dataset, use batch transform. With batch transform, you create a batch transform job using a trained model and the dataset, which must be stored in Amazon S3. Amazon SageMaker saves the inferences in an S3 bucket that you specify when you create the batch transform job. Batch transform manages all of the compute resources required to get inferences. This includes launching instances and deleting them after the batch transform job has completed. Batch transform manages interactions between the data and the model with an object within the instance node called an agent.\n",
    "\n",
    "Use batch transform when you:\n",
    "\n",
    "- Want to get inferences for an entire dataset and index them to serve inferences in real time\n",
    "\n",
    "- Don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "\n",
    "- Don't need the subsecond latency that Amazon SageMaker hosted endpoints provide\n",
    "\n",
    "You can also use batch transform to preprocess your data before using it to train a new model or generate inferences.\n",
    "\n",
    "The following diagram shows [the workflow of a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html):\n",
    "\n",
    "![image](https://docs.aws.amazon.com/sagemaker/latest/dg/images/batch-transform-v2.png)\n",
    "\n",
    "Now we have the classifier packaged up and are almost ready to use the model as a SageMaker model for inferencing. We would also need the serving codes that goes along with the classifier `model.tar.gz`. The serving script that will be put into the serving sklearn container requires an input argument `--model-dir` so that SageMaker can pass in the model location, and a function `model_fn` to deserialize and return the fitted classifier. We create a script below and save it as `sklearn_iris_serving.py`. \n",
    "\n",
    "\n",
    "Additionally, we also need to customize `predict_fn` and `output_fn` to accommodate the multi-value output. The `predict_fn` would need to return all the values in a numpy array of shape (m, n), where m is number of data point, and n is number of prediction output. For example, we would like the batch transform to output hard labels, and probability for each class, 2 values for binary classification use case, the output file should contain 3 output values for each data point. We will concatenate `clf.predict()` and `clf.predict_proba()` as below using `np.hstack()` in the `predict_fn`. The numpy array output from `predict_fn` would then pass to `output_fn` by the SageMaker Batch Transform to construct the final output based on the datatype `accept_type`, a user input to `model.transformer`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.hstack([clf.predict(X).reshape(-1, 1), clf.predict_proba(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sklearn_serving.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from sagemaker_containers.beta.framework import worker, encoders\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "#     parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = pickle.load(open(os.path.join(model_dir, \"model.pkl\"), 'rb'))\n",
    "    return clf\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Make inference against the model\n",
    "\n",
    "    We would like to return the probability in addition to the hard classification. \n",
    "    Note that the output should be of np.array type which is what the downstream function\n",
    "    output_fn expects.\n",
    "    \"\"\"\n",
    "    prediction = model.predict(input_data)\n",
    "    pred_prob = model.predict_proba(input_data)\n",
    "    return np.hstack([prediction.reshape(-1, 1), pred_prob])\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "    \n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({\"prediction\": row})\n",
    "\n",
    "        json_output = {\"instances\": instances}\n",
    "\n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(accept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a SageMaker Model for sklearn model. We need the model location (s3 path), SageMaker execution role, serving script as `entry_point` and the sklearn version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = SKLearnModel(model_data=model,\n",
    "                             role=role,\n",
    "                             entry_point=\"sklearn_serving.py\",\n",
    "                             framework_version=\"0.20.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data <a class=\"anchor\" id=\"prepare_input_data\"></a>\n",
    "We also upload the data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/x_dense.csv', X, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhrt ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload input data from local filesystem to S3\n",
    "test_csv_input_s3 = sagemaker_session.upload_data('./data/x_dense.csv', key_prefix=prefix + '/x_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform <a class=\"anchor\" id=\"batch_transform\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job with default <a class=\"anchor\" id=\"run_transform_job\"></a>\n",
    "Using the Transformer, run a transform job on the S3 input data. When creating the transformer, we need to specify `accept='text/csv'` which instruct the output to be of csv format. `content_type` in the `.transform()` call indicates that the input `batch_input_s3` is of csv type. We are also instructing the trnasformer to split the input data by `'Line'` and assemble them back to a single csv by `'Line'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_payload = 6, split_type = 'Line'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = sklearn_model.transformer(instance_count=1, instance_type='ml.m5.xlarge', \n",
    "                                         max_payload = 6, accept = 'text/csv', assemble_with = 'Line')\n",
    "# Start a transform job\n",
    "transformer.transform(test_csv_input_s3, content_type='text/csv', split_type = 'Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Output Data  <a class=\"anchor\" id=\"check_output_data\"></a>\n",
    "After the transform job has completed, download the output data from S3. For each file \"f\" in the input data, we have a corresponding file \"f.out\" containing the predicted labels from each input row. We can compare the predicted labels to the true labels saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/\n",
    "# Head to see what the batch output looks like\n",
    "!head batch_data/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
